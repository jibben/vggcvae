{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial, reduce\n",
    "import os.path\n",
    "import os\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.ndimage import imread\n",
    "\n",
    "from keras.layers import Flatten, Reshape, Input, Dense, Lambda, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D, ZeroPadding2D, UpSampling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, Callback\n",
    "from keras.metrics import binary_crossentropy\n",
    "from keras.engine.topology import Layer\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import keras.optimizers\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "K.set_floatx('float32')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are gonna play around with mnist data to start, just for proof of concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Variational Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the vgg16 building code from HW5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_vgg16(img_width, img_height, framework='tf'):\n",
    "\n",
    "    if framework == 'th':\n",
    "        # build the VGG16 network in Theano weight ordering mode\n",
    "        K.set_image_dim_ordering('th')\n",
    "    else:\n",
    "        # build the VGG16 network in Tensorflow weight ordering mode\n",
    "        K.set_image_dim_ordering('tf')\n",
    "        \n",
    "    model = Sequential()\n",
    "    if framework == 'th':\n",
    "        model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\n",
    "    else:\n",
    "        model.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height, 3)))\n",
    "        \n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv4_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv4_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv5_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv5_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv5_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a helper function to load a new vgg net (also adapted from HW5 code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_vgg(img_width, img_height, weights_path='vgg16_weights.h5'):\n",
    "    # path to the model weights files.\n",
    "    weights_path = 'vgg16_weights.h5'\n",
    "    th_model = build_vgg16(img_width, img_height, 'th')\n",
    "\n",
    "    # load the weights of the VGG16 networks\n",
    "    # (trained on ImageNet, won the ILSVRC competition in 2014)\n",
    "    # note: when there is a complete match between your model definition\n",
    "    # and your weight savefile, you can simply call model.load_weights(filename)\n",
    "    assert os.path.exists(weights_path), 'Model weights not found (see \"weights_path\" variable in script).'\n",
    "    f = h5py.File(weights_path)\n",
    "    for k in range(f.attrs['nb_layers']):\n",
    "        if k >= len(th_model.layers):\n",
    "            # we don't look at the last (fully-connected) layers in the savefile\n",
    "            break\n",
    "        g = f['layer_{}'.format(k)]\n",
    "        weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "        th_model.layers[k].set_weights(weights)\n",
    "    f.close()\n",
    "    print('Theano Model loaded.')\n",
    "    \n",
    "    # create tensorflow model and transfer weights from theano\n",
    "    tf_model = build_vgg16(img_width, img_height, 'tf')\n",
    "\n",
    "    for th_layer, tf_layer in zip(th_model.layers, tf_model.layers):\n",
    "        if th_layer.__class__.__name__ == 'Conv2D':\n",
    "          kernel, bias = th_layer.get_weights()\n",
    "          kernel = np.transpose(kernel, (2, 3, 1, 0))\n",
    "          tf_layer.set_weights([kernel, bias])\n",
    "        else:\n",
    "          tf_layer.set_weights(tf_layer.get_weights())\n",
    "        \n",
    "    return tf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom layer to calculate our VGG CVAE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need a different loss layer because we are gonna replace pixel-wise loss with feature-wise loss\n",
    "# because Keras isn't powerful to handle the loss for a CVAE\n",
    "# we have to create an actual loss layer to calculate it\n",
    "class VGG_CVAELossLayer(Layer):\n",
    "    def __init__(self, latent_dim, image_size, kl_lambda = 1, percep_lambda = 1, **kwargs):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.kl_l = kl_lambda\n",
    "        self.pc_l = percep_lambda\n",
    "        \n",
    "        self.is_placeholder = True\n",
    "        \n",
    "        super(VGG_CVAELossLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def calculate_kl(self, z_mean, z_log_sigma):\n",
    "        # this is the Kullback Libeler divergence between the\n",
    "        # distribution in latent space and the prior\n",
    "        latent_loss = - 0.5 * K.sum(1 + K.clip(z_log_sigma, 1e-10, 1e10) - K.clip(K.square(z_mean), 1e-10, 1e10) - K.clip(K.exp(z_log_sigma), 1e-10, 1e10), axis = -1)\n",
    "        \n",
    "        return latent_loss\n",
    "    \n",
    "    def calculate_perceploss(self, real, decoded):\n",
    "        return K.reduce_mean(K.subtract(real, decoded) ** 2, [1, 2, 3])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # first we will calculate kl\n",
    "        kl_div = self.calculate_kl(inputs[0][:,:self.latent_dim], inputs[1][:,:self.latent_dim])\n",
    "        \n",
    "        # then calculate perceptual losses\n",
    "        batch_size, _, _, _ = inputs[1].get_shape().as_list()\n",
    "        perceps = (len(inputs) - 1) / 2\n",
    "        p_loss = K.zeros(batch_size).astype('float32')\n",
    "        \n",
    "        for i in range(1, perceps):\n",
    "            p_loss += self.calculate_perceploss(inputs[i], inputs[i] + perceps)\n",
    "\n",
    "        p_loss = K.mean(p_loss, axis=-1)\n",
    "        \n",
    "        # this is the hacky way to calculate our loss\n",
    "        # we use the inputs given to calculate the loss\n",
    "        # and then return it for direct use for fitting\n",
    "        return (self.kl_l * kl_div + self.pc_l * p_loss)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to ensure we have the right amount of padding\n",
    "def build_padding(v_rem, h_rem, mult):\n",
    "    lay = None\n",
    "    \n",
    "    v_pad = 0\n",
    "    h_pad = 0\n",
    "    \n",
    "    if v_rem >= mult:\n",
    "        v_rem -= mult\n",
    "        v_pad = 1\n",
    "\n",
    "    if h_rem >= mult:\n",
    "        h_rem -= mult\n",
    "        h_pad = 1\n",
    "    \n",
    "    if h_pad or v_pad:\n",
    "        lay = ZeroPadding2D(padding=(v_pad, h_pad))\n",
    "        \n",
    "    return lay, v_rem, h_rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_sample(latent_dims, inputs):\n",
    "    z_mean = inputs[:,:latent_dims]\n",
    "    z_log_sigma = inputs[:,latent_dims:]\n",
    "\n",
    "    eps = K.random_normal(shape=(latent_dims,), mean=0.0, stddev=1.0, dtype='float32')\n",
    "    \n",
    "    return z_mean + (K.exp(z_log_sigma) * eps)\n",
    "\n",
    "def sample_output_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 2\n",
    "    shape[1] //= 2\n",
    "    return tuple(shape)\n",
    "    \n",
    "def get_layer_output(model, layer_num, learning_phase=True):\n",
    "    if learning_phase:\n",
    "        return K.function([model.layers[0].input, K.learning_phase()], [model.layers[layer_num].output])\n",
    "    else:\n",
    "        return K.function([model.layers[0].input], [model.layers[layer_num].output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the code to build the net itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VGG_CVAE(object):\n",
    "    \n",
    "    def __init__(self, image_size, latent_dim, int_dim = 256, num_k=32, k_size=4):\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.int_dim = int_dim\n",
    "        self.num_k = num_k\n",
    "        self.k_size = k_size\n",
    "        \n",
    "        self._build_net()\n",
    "        \n",
    "    def _build_net(self):\n",
    "        self._build_encoder()\n",
    "        self._build_sampler()\n",
    "        self._build_decoder()\n",
    "        self.kl_loss = VGG_CVAELossLayer(self.latent_dim, self.image_size, name='cvae_loss')\n",
    "        \n",
    "        vgg = build_vgg(self.image_size[0], self.image_size[1])\n",
    "        for layer in vgg.layers:\n",
    "            layer.trainable = False\n",
    "            \n",
    "        raw_inp = Input(shape=self.image_size)\n",
    "        \n",
    "        encoded = self.encoder(raw_inp)\n",
    "        sample = self.sample_layer(encoded)\n",
    "        decoded = self.decoder(sample)\n",
    "        \n",
    "        vgg_1 = get_layer_output(vgg, 1)\n",
    "        #vgg_2 = get_layer_output(vgg, 6)\n",
    "        vgg_3 = get_layer_output(vgg, 11)\n",
    "        #vgg_4 = get_layer_output(vgg, 18)\n",
    "        vgg_5 = get_layer_output(vgg, 25)\n",
    "        \n",
    "        real_1 = vgg_1(raw_inp)\n",
    "        real_3 = vgg_3(raw_inp)\n",
    "        real_5 = vgg_5(raw_inp)\n",
    "        \n",
    "        gen_1 = vgg_1(decoded)\n",
    "        gen_3 = vgg_3(decoded)\n",
    "        gen_5 = vgg_5(decoded)\n",
    "        \n",
    "        loss_layer = VGG_CVAELossLayer(self.latent_dim, self.image_size)([encoded, real_1, real_3, real_5, gen_1, gen_3, gen_5])\n",
    "        \n",
    "        self.model = Model(raw_inp, loss_layer)\n",
    "        \n",
    "        \n",
    "    def _build_encoder(self):\n",
    "        \n",
    "        c_params = {'padding' : 'same', 'activation' :  LeakyReLU(alpha=0.1)}\n",
    "        p_params= {'pool_size' : (2,2), 'strides' : (2,2), 'padding' : 'same'}\n",
    "    \n",
    "        inp = Input(shape=self.image_size)\n",
    "    \n",
    "        # just a series of convolutions and poolings to get the size down\n",
    "        x = Conv2D(self.num_k, (self.k_size, self.k_size), padding='same')(inp)\n",
    "        x = BatchNormalization(momentum=0.9)(x)\n",
    "        x = Activation(LeakyReLU(alpha=0.1))(x)\n",
    "        x = MaxPooling2D(**p_params)(x)\n",
    "    \n",
    "        x = Conv2D(self.num_k * 2, (self.k_size, self.k_size), padding='same')(x)\n",
    "        x = BatchNormalization(momentum=0.9)(x)\n",
    "        x = Activation(LeakyReLU(alpha=0.1))(x)\n",
    "        x = MaxPooling2D(**p_params)(x)\n",
    "    \n",
    "        x = Conv2D(self.num_k * 4, (self.k_size, self.k_size), padding='same')(x)\n",
    "        x = BatchNormalization(momentum=0.9)(x)\n",
    "        x = Activation(LeakyReLU(alpha=0.1))(x)\n",
    "        x = MaxPooling2D(**p_params)(x)\n",
    "    \n",
    "        x = Conv2D(self.num_k * 4, (self.k_size, self.k_size), padding='same')(x)\n",
    "        x = BatchNormalization(momentum=0.9)(x)\n",
    "        x = Activation(LeakyReLU(alpha=0.1))(x)\n",
    "        x = MaxPooling2D(**p_params)(x)\n",
    "    \n",
    "        # then we flatten, and have two consecutive dense layers\n",
    "        flat = Flatten()(x)\n",
    "        d = Dense(self.int_dim, activation='relu')(flat)\n",
    "        d = Dropout(0.2)(d)\n",
    "        encoded = Dense(self.latent_dim * 2, activation='sigmoid')(d)\n",
    "    \n",
    "        self.encoder = Model(inp, encoded)\n",
    "        \n",
    "    def _build_sampler(self):\n",
    "        '''layer to sample z from the latent space'''\n",
    "        # build a layer to sample from the given probability distribution\n",
    "        sample_func = partial(dist_sample, self.latent_dim)\n",
    "        # sometimes python is kind of dumb\n",
    "        # like right now, partial functions don't have names which messes up keras\n",
    "        # so let's manually add one\n",
    "        sample_func.__name__ = 'decoder_sample_func'\n",
    "        \n",
    "        self.sample_layer = Lambda(sample_func, output_shape = sample_output_shape)\n",
    "    \n",
    "    def _build_decoder(self):\n",
    "        \n",
    "        k_size = self.k_size\n",
    "        # figure out what dimension we want to start with\n",
    "        # and how much remainder we'll have (for padding)\n",
    "        v_dim = self.image_size[0] // 16\n",
    "        h_dim = self.image_size[1] // 16\n",
    "        v_rem = self.image_size[0] - (v_dim * 16) + 1\n",
    "        h_rem = self.image_size[1] - (h_dim * 16) + 1\n",
    "    \n",
    "        # get input\n",
    "        sample = Input(shape=(self.latent_dim,))\n",
    "    \n",
    "        # upscale with dense, use some dropout to combat mode collapse\n",
    "        x = Dense(self.int_dim, activation='relu')(sample)\n",
    "        x = Dense(self.num_k // 2 * v_dim * h_dim, activation = 'relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Reshape((v_dim, h_dim, self.num_k // 2))(x)\n",
    "    \n",
    "        # convolutinal upscaling layers\n",
    "        # padding logic will automatically pad such that the output shape is correct\n",
    "\n",
    "        x = Conv2DTranspose(self.num_k*4, (self.k_size, self.k_size), strides=(2,2), padding='same')(x)\n",
    "        x = BatchNormalization(momentum=0.9)(x)\n",
    "        x = Activation(LeakyReLU(alpha=0.1))(x)\n",
    "    \n",
    "        pad, v_rem, h_rem = build_padding(v_rem, h_rem, 8)\n",
    "        if pad:\n",
    "            x = pad(x)\n",
    "        \n",
    "        x = Conv2DTranspose(self.num_k*2, (self.k_size, self.k_size), strides=(2,2), padding='same')(x)\n",
    "        x = BatchNormalization(momentum=0.9)(x)\n",
    "        x = Activation(LeakyReLU(alpha=0.1))(x)\n",
    "    \n",
    "        pad, v_rem, h_rem = build_padding(v_rem, h_rem, 4)\n",
    "        if pad:\n",
    "            x = pad(x)\n",
    "    \n",
    "        x = Conv2DTranspose(self.num_k, (self.k_size, self.k_size), strides=(2,2), padding='same')(x)\n",
    "        x = BatchNormalization(momentum=0.9)(x)\n",
    "        x = Activation(LeakyReLU(alpha=0.1))(x)\n",
    "    \n",
    "        pad, v_rem, h_rem = build_padding(v_rem, h_rem, 2)\n",
    "        if pad:\n",
    "            x = pad(x)\n",
    "    \n",
    "        gen = Conv2DTranspose(self.num_k*4, (self.k_size, self.k_size), strides=(2,2), padding='same', activation='sigmoid')(x)\n",
    "    \n",
    "        self.decoder = Model(sample, gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[3,3,256,512]\n\t [[Node: conv4_1_1/random_uniform/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv4_1_1/random_uniform/RandomUniform, conv4_1_1/random_uniform/sub)]]\n\nCaused by op 'conv4_1_1/random_uniform/mul', defined at:\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-31-966afd9688e1>\", line 1, in <module>\n    cvae = VGG_CVAE((64,64,1), 64)\n  File \"<ipython-input-29-db739693f9ba>\", line 11, in __init__\n    self._build_net()\n  File \"<ipython-input-29-db739693f9ba>\", line 19, in _build_net\n    vgg = build_vgg(self.image_size[0], self.image_size[1])\n  File \"<ipython-input-3-49540505d1ff>\", line 4, in build_vgg\n    th_model = build_vgg16(img_width, img_height, 'th')\n  File \"<ipython-input-2-7650971b9888>\", line 36, in build_vgg16\n    model.add(Conv2D(512, (3, 3), activation='relu', name='conv4_1'))\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/models.py\", line 455, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/engine/topology.py\", line 528, in __call__\n    self.build(input_shapes[0])\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/layers/convolutional.py\", line 134, in build\n    constraint=self.kernel_constraint)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/engine/topology.py\", line 364, in add_weight\n    weight = K.variable(initializer(shape), dtype=K.floatx(), name=name)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/initializers.py\", line 205, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3146, in random_uniform\n    dtype=dtype, seed=seed)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py\", line 245, in random_uniform\n    return math_ops.add(rnd * (maxval - minval), minval, name=name)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 794, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1015, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1625, in _mul\n    result = _op_def_lib.apply_op(\"Mul\", x=x, y=y, name=name)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[3,3,256,512]\n\t [[Node: conv4_1_1/random_uniform/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv4_1_1/random_uniform/RandomUniform, conv4_1_1/random_uniform/sub)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3,3,256,512]\n\t [[Node: conv4_1_1/random_uniform/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv4_1_1/random_uniform/RandomUniform, conv4_1_1/random_uniform/sub)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-966afd9688e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG_CVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-db739693f9ba>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image_size, latent_dim, int_dim, num_k, k_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-db739693f9ba>\u001b[0m in \u001b[0;36m_build_net\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG_CVAELossLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cvae_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mvgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-49540505d1ff>\u001b[0m in \u001b[0;36mbuild_vgg\u001b[0;34m(img_width, img_height, weights_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layer_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'param_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nb_params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mth_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Theano Model loaded.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mparam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(ops)\u001b[0m\n\u001b[1;32m   1945\u001b[0m     \"\"\"\n\u001b[1;32m   1946\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1947\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1948\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1949\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m()\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muninitialized_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3,3,256,512]\n\t [[Node: conv4_1_1/random_uniform/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv4_1_1/random_uniform/RandomUniform, conv4_1_1/random_uniform/sub)]]\n\nCaused by op 'conv4_1_1/random_uniform/mul', defined at:\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-31-966afd9688e1>\", line 1, in <module>\n    cvae = VGG_CVAE((64,64,1), 64)\n  File \"<ipython-input-29-db739693f9ba>\", line 11, in __init__\n    self._build_net()\n  File \"<ipython-input-29-db739693f9ba>\", line 19, in _build_net\n    vgg = build_vgg(self.image_size[0], self.image_size[1])\n  File \"<ipython-input-3-49540505d1ff>\", line 4, in build_vgg\n    th_model = build_vgg16(img_width, img_height, 'th')\n  File \"<ipython-input-2-7650971b9888>\", line 36, in build_vgg16\n    model.add(Conv2D(512, (3, 3), activation='relu', name='conv4_1'))\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/models.py\", line 455, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/engine/topology.py\", line 528, in __call__\n    self.build(input_shapes[0])\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/layers/convolutional.py\", line 134, in build\n    constraint=self.kernel_constraint)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/engine/topology.py\", line 364, in add_weight\n    weight = K.variable(initializer(shape), dtype=K.floatx(), name=name)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/initializers.py\", line 205, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3146, in random_uniform\n    dtype=dtype, seed=seed)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py\", line 245, in random_uniform\n    return math_ops.add(rnd * (maxval - minval), minval, name=name)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 794, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1015, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1625, in _mul\n    result = _op_def_lib.apply_op(\"Mul\", x=x, y=y, name=name)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[3,3,256,512]\n\t [[Node: conv4_1_1/random_uniform/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv4_1_1/random_uniform/RandomUniform, conv4_1_1/random_uniform/sub)]]\n"
     ]
    }
   ],
   "source": [
    "cvae = VGG_CVAE((64,64,1), 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a helper function to demo a model on a given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def demo_model(model, data, shape, show_latent=False):\n",
    "    e = model.layers[1]\n",
    "    d = model.layers[2]\n",
    "    \n",
    "    batch_size = data.shape[0]\n",
    "    \n",
    "    vecs = e.predict(data, batch_size = batch_size, verbose=0)\n",
    "    pred = d.predict(vecs, batch_size = batch_size, verbose=0)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        fig = plt.figure()\n",
    "        a=fig.add_subplot(1,2,1)\n",
    "        a.spines['top'].set_color('none')\n",
    "        a.spines['bottom'].set_color('none')\n",
    "        a.spines['left'].set_color('none')\n",
    "        a.spines['right'].set_color('none')\n",
    "        a.tick_params(labelcolor='w', top='off', bottom='off', left='off', right='off')\n",
    "        img = plt.imshow(data[i].reshape(shape))\n",
    "        a.set_title('input')\n",
    "        \n",
    "        a=fig.add_subplot(1,2,2)\n",
    "        a.spines['top'].set_color('none')\n",
    "        a.spines['bottom'].set_color('none')\n",
    "        a.spines['left'].set_color('none')\n",
    "        a.spines['right'].set_color('none')\n",
    "        a.tick_params(labelcolor='w', top='off', bottom='off', left='off', right='off')\n",
    "        img = plt.imshow(pred[i].reshape(shape))\n",
    "        a.set_title('decoded')\n",
    "        if(show_latent):\n",
    "            a.set_xlabel(vecs[i])\n",
    "        \n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ImageSaver(Callback):\n",
    "    ''' Keras Callback to save demo images after each epoch'''\n",
    "    def __init__(self, image_path, images, shape, period=1):\n",
    "        self.images = images\n",
    "        self.batch_size = images.shape[0]\n",
    "        self.image_path = image_path\n",
    "        self.shape = shape\n",
    "        self.period = period\n",
    "        \n",
    "        self.dpi = 128\n",
    "        self.fig_size = ((shape[0] * 4) // self.dpi, int(shape[1] * 1.5 * self.batch_size) // self.dpi)\n",
    "        \n",
    "        super(ImageSaver, self).__init__()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.period == 0:\n",
    "            vecs = self.model.layers[1].predict(self.images, batch_size=self.batch_size, verbose=0)\n",
    "            pred = self.model.layers[2].predict(vecs, batch_size=self.batch_size, verbose=0)\n",
    "        \n",
    "            fig = plt.figure(figsize=self.fig_size)\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                a = fig.add_subplot(self.batch_size, 2, i * 2 + 1)\n",
    "                a.spines['top'].set_color('none')\n",
    "                a.spines['bottom'].set_color('none')\n",
    "                a.spines['left'].set_color('none')\n",
    "                a.spines['right'].set_color('none')\n",
    "                a.tick_params(labelcolor='w', top='off', bottom='off', left='off', right='off')\n",
    "                img = plt.imshow(self.images[i].reshape(shape))\n",
    "                a.set_title('input')\n",
    "                \n",
    "                a = fig.add_subplot(self.batch_size, 2, i * 2 + 2)\n",
    "                a.spines['top'].set_color('none')\n",
    "                a.spines['bottom'].set_color('none')\n",
    "                a.spines['left'].set_color('none')\n",
    "                a.spines['right'].set_color('none')\n",
    "                a.tick_params(labelcolor='w', top='off', bottom='off', left='off', right='off')\n",
    "                img = plt.imshow(pred[i].reshape(shape))\n",
    "                a.set_title('decoded')\n",
    "            \n",
    "            fig.show()\n",
    "            fig.savefig(self.image_path.format(epoch=epoch), dpi=self.dpi)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using this on mnist data, just for a proof of concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape = (28,28,1)\n",
    "latent_dim = 64\n",
    "\n",
    "model = build_cvae(shape, latent_dim, vgg=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28 ,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shape = (28,28,1)\n",
    "latent_dim = 2\n",
    "\n",
    "model = build_cvae(shape, latent_dim, vgg=False)\n",
    "model.fit(x_train, np.repeat(1, 60000), epochs=5, batch_size=64, shuffle=True, validation_data=(x_test, np.repeat(1,10000)), callbacks=[TensorBoard(log_dir='/tmp/ae')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And show what it does to MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demo_model(model, x_test[90:100], (28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not bad considering we are effectively compressing each image to two dimensions! But let's move on to try it with something a bit more interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about flowers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape = (64,64,3)\n",
    "latent_dim = 128\n",
    "int_dim = 256\n",
    "nb_train = 8189\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "train_data_dir = '../flowers/train'\n",
    "\n",
    "flower_model = build_cvae(shape, latent_dim, int_dim=int_dim, optimizer='adamax')\n",
    "\n",
    "flower_train_generator = ImageDataGenerator(rescale=1/255, zoom_range=0.2, horizontal_flip=True).flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(shape[0], shape[1]),\n",
    "        batch_size = batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "flower_checkpoint = ModelCheckpoint('./models/flower_weights..hdf5', monitor='loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = flower_model.fit_generator(\n",
    "        flower_train_generator,\n",
    "        steps_per_epoch = nb_train // batch_size,\n",
    "        epochs = epochs,\n",
    "        validation_data = None,\n",
    "        verbose = 1,\n",
    "        initial_epoch = 0,\n",
    "        callbacks=[TensorBoard(log_dir='/tmp/flowers')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demo_model(flower_model, next(flower_train_generator)[0], (64,64, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, not super happy with the mode collapse we're seeing. Let's try something with more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How about faces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape = (128,128,3)\n",
    "latent_dim = 256\n",
    "int_dim = 500\n",
    "nb_train = 162770\n",
    "nb_validation = 15000\n",
    "batch_size = 8\n",
    "epochs = 50\n",
    "\n",
    "train_data_dir = '../faces/celebs/train'\n",
    "validation_data_dir = '../faces/celebs/validation'\n",
    "test_data_dir = '../faces/celebs/test'\n",
    "face_model = build_cvae(shape, latent_dim, int_dim=int_dim, optimizer='adamax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 162770 images belonging to 2 classes.\n",
      "Found 19867 images belonging to 2 classes.\n",
      "Found 19962 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# we have our setup such that we have the first class with no data\n",
    "# and the second class with all the data\n",
    "# this way, we can actually use a ImageDataGenerator and get the [1]\n",
    "# we expect for our fitting, as was done above\n",
    "\n",
    "face_train_generator = ImageDataGenerator(rescale=1/255).flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(shape[0], shape[1]),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "face_validation_generator = ImageDataGenerator(rescale=1/255).flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(shape[0], shape[1]),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "face_test_generator = ImageDataGenerator(rescale=1/255).flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(shape[0], shape[1]),\n",
    "        batch_size=16,\n",
    "        class_mode=None)\n",
    "\n",
    "face_checkpoint = ModelCheckpoint('./models/face_weights.hdf5', monitor='loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=5)\n",
    "face_saver = ImageSaver('./images/face-{epoch:02d}.png', next(face_test_generator), shape, period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    429\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         \u001b[0mdims_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \"\"\"\n\u001b[0;32m--> 502\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'Tensor' object is not iterable.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not iterable.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    799\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;31m# Treat as a singleton dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    377\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m       if (not isinstance(value, compat.bytes_or_text_types)\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'Tensor'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-21dcf804d631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0minitial_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/tmp/faces'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_saver\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1774\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1775\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    999\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m                 self.total_loss)\n\u001b[0m\u001b[1;32m   1002\u001b[0m             \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtraining_updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0;31m# Gets loss and metrics. Updates weights at each call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, params, constraints, loss)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clipnorm'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclipnorm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(loss, variables)\u001b[0m\n\u001b[1;32m   2106\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2107\u001b[0m     \"\"\"\n\u001b[0;32m-> 2108\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0mout_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZerosLike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                 \u001b[0mout_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZerosLikeOutsideLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_grad\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mZerosLikeOutsideLoop\u001b[0;34m(op, index)\u001b[0m\n\u001b[1;32m   1305\u001b[0m     \u001b[0mswitch_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbranch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[0mzeros_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswitch_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1307\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzeros_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1355\u001b[0;31m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1356\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mfill\u001b[0;34m(dims, value, name)\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m   \"\"\"\n\u001b[0;32m-> 1318\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fill\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1319\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    761\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    762\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2325\u001b[0m     ret = Operation(node_def, self, inputs=inputs, output_types=dtypes,\n\u001b[1;32m   2326\u001b[0m                     \u001b[0mcontrol_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontrol_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2327\u001b[0;31m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0m\u001b[1;32m   2328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2329\u001b[0m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1224\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1227\u001b[0m     \u001b[0;31m# Add this op to the current control flow context:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_control_flow_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_control_flow_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibben/anaconda3/envs/deepcv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_extract_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mframe_globals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = face_model.fit_generator(\n",
    "        face_train_generator,\n",
    "        steps_per_epoch = nb_train // batch_size,\n",
    "        epochs = epochs,\n",
    "        validation_data = face_validation_generator,\n",
    "        validation_steps = nb_validation // batch_size,\n",
    "        verbose = 0,\n",
    "        initial_epoch = 0,\n",
    "        callbacks=[TensorBoard(log_dir='/tmp/faces'), face_checkpoint, face_saver]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "demo_model(face_model, next(face_train_generator)[0], (128,128,3), show_latent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e = face_model.layers[1]\n",
    "d = face_model.layers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = next(face_train_generator)[0]\n",
    "    \n",
    "batch_size = data.shape[0]\n",
    "    \n",
    "vecs = e.predict(data, batch_size = batch_size, verbose=0)\n",
    "pred = d.predict(vecs, batch_size = batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    show_latent = False\n",
    "    for i in range(batch_size):\n",
    "        fig = plt.figure()\n",
    "        a=fig.add_subplot(1,2,1)\n",
    "        a.spines['top'].set_color('none')\n",
    "        a.spines['bottom'].set_color('none')\n",
    "        a.spines['left'].set_color('none')\n",
    "        a.spines['right'].set_color('none')\n",
    "        a.tick_params(labelcolor='w', top='off', bottom='off', left='off', right='off')\n",
    "        img = plt.imshow(data[i].reshape(shape))\n",
    "        a.set_title('input')\n",
    "        \n",
    "        a=fig.add_subplot(1,2,2)\n",
    "        a.spines['top'].set_color('none')\n",
    "        a.spines['bottom'].set_color('none')\n",
    "        a.spines['left'].set_color('none')\n",
    "        a.spines['right'].set_color('none')\n",
    "        a.tick_params(labelcolor='w', top='off', bottom='off', left='off', right='off')\n",
    "        img = plt.imshow(pred[i].reshape(shape))\n",
    "        a.set_title('decoded')\n",
    "        if(show_latent):\n",
    "            a.set_xlabel(vecs[i])\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    batch_size = 1\n",
    "    vecs = np.random.uniform(size=(latent_dim*2,))\n",
    "    pred = d.predict(rand, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    show_latent = False\n",
    "    for i in range(batch_size):\n",
    "        fig = plt.figure()\n",
    "        a=fig.add_subplot(1,2,1)\n",
    "        a.spines['top'].set_color('none')\n",
    "        a.spines['bottom'].set_color('none')\n",
    "        a.spines['left'].set_color('none')\n",
    "        a.spines['right'].set_color('none')\n",
    "        a.tick_params(labelcolor='w', top='off', bottom='off', left='off', right='off')\n",
    "        img = plt.imshow(data[i].reshape(shape))\n",
    "        a.set_title('input')\n",
    "        \n",
    "        a=fig.add_subplot(1,2,2)\n",
    "        a.spines['top'].set_color('none')\n",
    "        a.spines['bottom'].set_color('none')\n",
    "        a.spines['left'].set_color('none')\n",
    "        a.spines['right'].set_color('none')\n",
    "        a.tick_params(labelcolor='w', top='off', bottom='off', left='off', right='off')\n",
    "        img = plt.imshow(pred[i].reshape(shape))\n",
    "        a.set_title('decoded')\n",
    "        if(show_latent):\n",
    "            a.set_xlabel(vecs[i])\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_generator(train_dir, batch_size):\n",
    "    files = os.listdir(train_dir)\n",
    "    \n",
    "    x_train = []\n",
    "    \n",
    "    while 1:\n",
    "        \n",
    "        for fname in os.listdir(train_dir):\n",
    "            x_train.append(imread(os.path.join(train_dir, fname)))\n",
    "            if len(x_train) >= batch_size:\n",
    "                print(x_train)\n",
    "                yield np.array(x_train), np.repeat(1, batch_size)\n",
    "                x_train = []\n",
    "            \n",
    "def valid_generator(valid_dir):\n",
    "    files = os.listdir(valid_dir)\n",
    "    \n",
    "    while 1:\n",
    "        for fname in files:\n",
    "            x_valid = imread(os.path.join(train_dir, fname))\n",
    "            yield x_valid, np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = train_generator('../scrape/flickr/abstract_art', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "train_dir = '../scrape/flickr/watercolor'\n",
    "\n",
    "for fname in os.listdir(train_dir):\n",
    "    try:\n",
    "        img = Image.open(os.path.join(train_dir, fname)).resize((128, 128))\n",
    "        x_train.append(np.array(img).reshape((128,128,3))/256.)\n",
    "    except:\n",
    "        print(fname)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.repeat(1, x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape = (128,128,3)\n",
    "latent_dim = 128\n",
    "int_dim = 256\n",
    "\n",
    "art_model = build_cvae(shape, latent_dim, int_dim=int_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "art_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "art_model.fit(x_train, y_train, epochs=20, batch_size=6, shuffle=True, validation_data=None, callbacks=[TensorBoard(log_dir='/tmp/ae')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e = art_model.layers[1]\n",
    "d = art_model.layers[2]\n",
    "\n",
    "x_train[100:108]\n",
    "\n",
    "vecs = e.predict(x_train, batch_size=8, verbose=0)\n",
    "print('vecs')\n",
    "pred = d.predict(vecs, batch_size=8, verbose=0)\n",
    "\n",
    "for i in range(10):\n",
    "    plt.imshow(x_train[i].reshape((128,128,3)))\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "    print(vecs[i])\n",
    "    plt.imshow(pred[i].reshape((128,128,3)))\n",
    "    plt.axis('on')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e = face_model.layers[1]\n",
    "d = face_model.layers[2]\n",
    "\n",
    "inp = next(face_train_generator)\n",
    "\n",
    "vecs = e.predict(inp[0], batch_size=8, verbose=0)\n",
    "pred = d.predict(vecs, batch_size=8, verbose=0)\n",
    "\n",
    "for i in range(10):\n",
    "    plt.imshow(inp[0][i].reshape((178,218,3)))\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "    print(vecs[i])\n",
    "    plt.imshow(pred[i].reshape((178,218,3)))\n",
    "    plt.axis('on')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's try using a pre-trained VGG to help with the encoding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG net from hw4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For this architecture, we already have all the convolutions from the VGGnet\n",
    "# we just have to shrink it to latent_dim size!\n",
    "def build_vgg_encoder(vggnet, shape, latent_dim, num_k=64, k_size=3, int_dim=256):\n",
    "    \n",
    "    vgg_encoder = build_vgg(shape[0], shape[1])\n",
    "    \n",
    "    encoder_top = Sequential()\n",
    "    encoder_top.add(MaxPooling2D((2, 2), strides=(2, 2), input_shape = vggnet.output_shape[1:]))\n",
    "    encoder_top.add(Flatten())\n",
    "    encoder_top.add(Dense(int_dim, activation='relu'))\n",
    "    encoder_top.add(Dropout(0.25))\n",
    "    encoder_top.add(Dense(latent_dim * 2))\n",
    "    \n",
    "    vggnet.add(encoder_top)\n",
    "   \n",
    "    return vgg_encoder\n",
    "\n",
    "def build_cvae(img_shape, image_size, latent_dim, int_dim=256, num_k=64, optimizer='adagrad'):\n",
    "    raw_inp = Input(shape=(shape[0], shape[1], shape[2]))\n",
    "\n",
    "    encoder = build_encoder(shape, latent_dim, num_k=num_k, int_dim=int_dim)\n",
    "    encoded = encoder(raw_inp)\n",
    "\n",
    "    decoder = build_decoder(shape, latent_dim, num_k=num_k, int_dim=int_dim)\n",
    "    decoded = decoder(encoded)\n",
    "\n",
    "\n",
    "    flat_raw = Flatten()(raw_inp)\n",
    "    flat_decoded = Flatten()(decoded)\n",
    "    loss = CVAELossLayer(latent_dim, image_size, name='cvae_loss')([flat_raw, encoded, flat_decoded])\n",
    "\n",
    "    model = Model(inputs=[raw_inp], outputs=[loss])\n",
    "    model.compile(optimizer=optimizer, loss={'cvae_loss' : lambda y_true, y_pred: y_pred})\n",
    "    return model\n",
    "\n",
    "def build_vgg_cvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, I'm not too happy with the mode collapse from CVAE's. Let's try making a GAN now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_generator(output_shape, latent_dims, num_k=64, k_size=5, int_dim=256):\n",
    "    inp = Input(shape=(latent_dims,))\n",
    "    x = Dense(int_dim, activation='relu')(inp)\n",
    "    \n",
    "    v_dim = output_shape[0] // 4\n",
    "    h_dim = output_shape[1] // 4\n",
    "    v_rem = output_shape[0] - (v_dim * 4)\n",
    "    h_rem = output_shape[1] - (h_dim * 4)\n",
    "    \n",
    "    x = Dense(num_k // 2 * v_dim * h_dim, activation = 'relu')(x)\n",
    "    x = Reshape((v_dim, h_dim, num_k // 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    int_shape = (output_shape[0], output_shape[1], num_k)\n",
    "    params = {'activation' : 'relu', 'padding' : 'valid'}\n",
    "    p_params= {'pool_size' : (2,2), 'strides' : (2,2), 'padding' : 'same'}\n",
    "    \n",
    "    x = Conv2DTranspose(num_k, (k_size, k_size), strides=(1,1), activation = 'relu', padding='same')(x)\n",
    "\n",
    "    x = Conv2DTranspose(num_k*4, (k_size, k_size), strides=(2,2), padding='same')(x)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    pad, v_rem, h_rem = build_padding(v_rem, h_rem, 8)\n",
    "    if pad:\n",
    "        x = pad(x)\n",
    "        \n",
    "    x = Conv2DTranspose(num_k*2, (k_size, k_size), strides=(2,2), padding='same')(x)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    pad, v_rem, h_rem = build_padding(v_rem, h_rem, 4)\n",
    "    if pad:\n",
    "        x = pad(x)\n",
    "    \n",
    "    '''x = Conv2DTranspose(num_k, (k_size, k_size), strides=(2,2), padding='same')(x)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    pad, v_rem, h_rem = build_padding(v_rem, h_rem, 2)\n",
    "    if pad:\n",
    "        x = pad(x)'''\n",
    "    \n",
    "    x = Conv2DTranspose(num_k, (k_size, k_size), strides=(2,2), padding='same')(x)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = MaxPooling2D(**p_params)(x)\n",
    "    gen = Conv2D(output_shape[2], (4,4), padding = 'same', activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(inp, gen)\n",
    "    \n",
    "    pad, v_rem, h_rem = build_padding(v_rem, h_rem, 4)\n",
    "    if pad:\n",
    "        x = pad(x)\n",
    "        \n",
    "    x = Conv2DTranspose(num_k*2, (k_size, k_size), strides=(2,2), activation = 'relu', padding='valid')(x)\n",
    "    pad, v_rem, h_rem = build_padding(v_rem, h_rem, 2)\n",
    "    if pad:\n",
    "        x = pad(x)\n",
    "        \n",
    "    x = Conv2DTranspose(num_k, (k_size, k_size), strides=(2,2), activation = 'relu', padding='valid')(x)\n",
    "    pad, v_rem, h_rem = build_padding(v_rem, h_rem, 1)\n",
    "    if pad:\n",
    "        x = pad(x)\n",
    "        \n",
    "    x = MaxPooling2D(**p_params)(x)\n",
    "    gen = Conv2D(output_shape[2], (4,4), padding = 'valid', activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(inp, gen)\n",
    "\n",
    "\n",
    "def build_adversary(shape, num_k=64, k_size=4, int_dim=256):\n",
    "    c_params = {'padding' : 'same', 'activation' : LeakyReLU(alpha=0.2)}\n",
    "    p_params= {'pool_size' : (2,2), 'strides' : (2,2), 'padding' : 'same'}\n",
    "    \n",
    "    adv = Sequential()\n",
    "    adv.add(Conv2D(num_k, (k_size, k_size), input_shape=shape, **c_params))\n",
    "    adv.add(MaxPooling2D(**p_params))\n",
    "    adv.add(Dropout(0.4))\n",
    "    adv.add(Conv2D(num_k*2, (k_size, k_size), **c_params))\n",
    "    adv.add(MaxPooling2D(**p_params))\n",
    "    adv.add(Dropout(0.4))\n",
    "    adv.add(Conv2D(num_k*4, (k_size, k_size), **c_params))\n",
    "    adv.add(MaxPooling2D(**p_params))\n",
    "    adv.add(Dropout(0.4))\n",
    "    \n",
    "    adv.add(Flatten())\n",
    "    adv.add(Dense(int_dim, activation='relu'))\n",
    "    adv.add(Dense(2, activation='sigmoid'))\n",
    "    \n",
    "    return adv\n",
    "\n",
    "def build_GAN(shape, latent_dim, num_k=64, k_size=5, int_dim=256, g_opt='adamax', a_opt='adamax', gan_opt='adamax'):\n",
    "    generator = build_generator(shape, latent_dim, k_size=k_size, num_k=num_k, int_dim=int_dim)\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=g_opt)\n",
    "    \n",
    "    adversary = build_adversary(shape, num_k=num_k, k_size=k_size, int_dim=int_dim)\n",
    "    adversary.compile(loss='categorical_crossentropy', optimizer=a_opt)\n",
    "    \n",
    "    gan = Sequential()\n",
    "    gan.add(generator)\n",
    "    gan.add(adversary)\n",
    "    \n",
    "    gan.compile(loss='binary_crossentropy', optimizer=gan_opt)\n",
    "    \n",
    "    return generator, adversary, gan\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def demo_images(imgs, shape, filename=None):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    to_show = min(math.floor(math.sqrt(imgs.shape[0])), 4)\n",
    "    for i in range(to_show**2):\n",
    "        a = fig.add_subplot(to_show, to_show, i+1)\n",
    "        a.spines['top'].set_color('none')\n",
    "        a.spines['bottom'].set_color('none')\n",
    "        a.spines['left'].set_color('none')\n",
    "        a.spines['right'].set_color('none')\n",
    "        a.tick_params(labelcolor='w', top='off', bottom='off', left='off', right='off')\n",
    "        \n",
    "        if len(shape) == 2:\n",
    "            img = plt.imshow(imgs[i].reshape(shape), cmap=plt.get_cmap('gray'))\n",
    "        else:\n",
    "            img = plt.imshow(imgs[i].reshape(shape))\n",
    "    plt.tight_layout()\n",
    "    if filename:\n",
    "        fig.savefig(filename)\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "def fit_gan(gen, adv, gan, data_stream, epochs, steps_per_epoch, latent_dims, shape):\n",
    "    # because we are doing our own training effectively\n",
    "    # we gotta keep track of losses independant of tensorflow\n",
    "    adv_loss = []\n",
    "    gan_loss = []\n",
    "    \n",
    "    if shape[2] == 1:\n",
    "        shape = (shape[0], shape[1])\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        adv_loss_ = []\n",
    "        gan_loss_ = []\n",
    "        for step in range(steps_per_epoch):\n",
    "            real_imgs = next(data_stream)\n",
    "            cur_batch_size = real_imgs.shape[0]\n",
    "            \n",
    "            # make batches\n",
    "            inp = np.random.uniform(-1.0, 1.0, size=(cur_batch_size, latent_dims)).astype('float32')\n",
    "            gen_imgs = gen.predict(inp)\n",
    "\n",
    "            # first let's train the adversary a bit\n",
    "            try:\n",
    "                X_batch = np.concatenate((real_imgs, gen_imgs))\n",
    "            except Exception as e:\n",
    "                print(real_imgs.shape)\n",
    "                print(gen_imgs.shape)\n",
    "                raise e\n",
    "            y_batch = np.zeros([2*cur_batch_size,2])\n",
    "            y_batch[cur_batch_size:,0] = 1\n",
    "            y_batch[:cur_batch_size,1] = 1\n",
    "\n",
    "            adv_loss_.append(adv.train_on_batch(X_batch, y_batch))\n",
    "\n",
    "            # now we can train the whole GAN\n",
    "            gaussian_noise = np.random.uniform(-1.0, 1.0, size=(cur_batch_size, latent_dims)).astype('float32')\n",
    "            y_g = np.zeros([cur_batch_size,2])\n",
    "            y_g[:,1] = 1\n",
    "            \n",
    "            gan_loss_.append(gan.train_on_batch(gaussian_noise, y_g))    \n",
    "        \n",
    "        adv_loss.append(sum(adv_loss_) / len(adv_loss_))\n",
    "        gan_loss.append(sum(gan_loss_) / len(gan_loss_))\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            demo_images(gen_imgs, shape, 'imgs{}.png'.format(i))\n",
    "            print(adv_loss)\n",
    "            print(gan_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape = (28,28,1)\n",
    "latent_dim = 100\n",
    "batch_size = 128\n",
    "\n",
    "g_opt = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=3e-8)\n",
    "a_opt = keras.optimizers.RMSprop(lr=0.0008, clipvalue=1.0, decay=6e-8)\n",
    "gan_opt = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=3e-8)\n",
    "gen, adv, gan = build_GAN(shape, latent_dim, g_opt=g_opt, a_opt=a_opt, gan_opt=gan_opt)\n",
    "\n",
    "def mnist_data_gen(batch_size):\n",
    "    (x_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "    \n",
    "    x_len = len(x_train)\n",
    "\n",
    "    while 1:\n",
    "        for i in range(x_len // batch_size):\n",
    "            end_idx = min((i+1) * batch_size, x_len)\n",
    "            yield x_train[i * batch_size : end_idx, :, :, :]\n",
    "            \n",
    "            \n",
    "epochs = 100\n",
    "train_size = 8189\n",
    "steps_per_epoch = train_size // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit_gan(gen, adv, gan, mnist_data_gen(batch_size), epochs, steps_per_epoch, latent_dim, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape = (64,64,1)\n",
    "latent_dim = 100\n",
    "batch_size = 32\n",
    "\n",
    "g_opt = keras.optimizers.RMSprop(lr=0.0002, clipvalue=1.0, decay=3e-8)\n",
    "a_opt = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=6e-8)\n",
    "gan_opt = keras.optimizers.RMSprop(lr=0.0002, clipvalue=1.0, decay=3e-8)\n",
    "\n",
    "#g_opt = keras.optimizers.Adamax(lr=0.001, clipvalue=1.0)\n",
    "#a_opt = keras.optimizers.Adamax(lr=0.002, clipvalue=1.0)\n",
    "#gan_opt = keras.optimizers.Adamax(lr=0.001, clipvalue=1.0)\n",
    "\n",
    "f_gen, f_adv, f_gan = build_GAN(shape, latent_dim, g_opt=g_opt, a_opt=a_opt, gan_opt=gan_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "train_size = 8189\n",
    "steps_per_epoch = train_size // batch_size\n",
    "train_data_dir = '../flowers/train'\n",
    "\n",
    "flower_train_generator = ImageDataGenerator(rescale=1/255).flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(shape[0], shape[1]),\n",
    "        color_mode='grayscale',\n",
    "        batch_size = batch_size,\n",
    "        class_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit_gan(f_gen, f_adv, f_gan, flower_train_generator, epochs, steps_per_epoch, latent_dim, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape = (128,128,3)\n",
    "latent_dims = 128\n",
    "batch_size = 16\n",
    "\n",
    "g_opt = keras.optimizers.RMSprop(lr=0.0008. decay=2e-8)\n",
    "a_opt = keras.optimizers.RMSprop(lr=0.0016, decay=4e-8)\n",
    "\n",
    "gen, adv, gan = build_GAN(shape, latent_dim, g_opt=g_opt, a_opt=a_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GANs are a bit harder to train too, so we won't use built in fitting\n",
    "epochs = 50\n",
    "train_size = 100000\n",
    "\n",
    "steps_per_epoch = train_size // batch_size\n",
    "\n",
    "train_data_dir = '../faces/celebs/train'\n",
    "\n",
    "real_data_gen = ImageDataGenerator(rescale=1/255).flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(shape[0], shape[1]),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='none')\n",
    "\n",
    "fit_gan(adv, gan, real_data_gen, batch_size, epochs,\n",
    "        steps_per_epoch, latent_dims, shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepcv)",
   "language": "python",
   "name": "deepcv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
